{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197f1adf",
   "metadata": {},
   "source": [
    "Here we want to build a model. We decide on building an LSTM for the following reasons: \n",
    "- Need to capture long-term information in the model.\n",
    "- Other model choices would not be appropriate (due to non-staionary time series).\n",
    "- Complexity of time series can be properly captured.\n",
    "\n",
    "We will first build a model for all states, then build a model per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3bf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import ml_utils\n",
    "import mlflow\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e8c2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>state</th><th>a</th><th>b</th><th>o</th><th>ab</th><th>all</th></tr><tr><td>date</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2006-01-01</td><td>&quot;Johor&quot;</td><td>19</td><td>20</td><td>45</td><td>3</td><td>87</td></tr><tr><td>2006-01-02</td><td>&quot;Johor&quot;</td><td>4</td><td>3</td><td>6</td><td>2</td><td>15</td></tr><tr><td>2006-01-03</td><td>&quot;Johor&quot;</td><td>2</td><td>2</td><td>4</td><td>0</td><td>8</td></tr><tr><td>2006-01-04</td><td>&quot;Johor&quot;</td><td>7</td><td>11</td><td>12</td><td>3</td><td>33</td></tr><tr><td>2006-01-05</td><td>&quot;Johor&quot;</td><td>3</td><td>8</td><td>8</td><td>1</td><td>20</td></tr><tr><td>2006-01-06</td><td>&quot;Johor&quot;</td><td>2</td><td>0</td><td>2</td><td>0</td><td>4</td></tr><tr><td>2006-01-07</td><td>&quot;Johor&quot;</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>2006-01-08</td><td>&quot;Johor&quot;</td><td>20</td><td>27</td><td>30</td><td>9</td><td>86</td></tr><tr><td>2006-01-09</td><td>&quot;Johor&quot;</td><td>0</td><td>1</td><td>1</td><td>1</td><td>3</td></tr><tr><td>2006-01-10</td><td>&quot;Johor&quot;</td><td>1</td><td>0</td><td>1</td><td>0</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 7)\n",
       "┌────────────┬───────┬─────┬─────┬─────┬─────┬─────┐\n",
       "│ date       ┆ state ┆ a   ┆ b   ┆ o   ┆ ab  ┆ all │\n",
       "│ ---        ┆ ---   ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- │\n",
       "│ date       ┆ str   ┆ i64 ┆ i64 ┆ i64 ┆ i64 ┆ i64 │\n",
       "╞════════════╪═══════╪═════╪═════╪═════╪═════╪═════╡\n",
       "│ 2006-01-01 ┆ Johor ┆ 19  ┆ 20  ┆ 45  ┆ 3   ┆ 87  │\n",
       "│ 2006-01-02 ┆ Johor ┆ 4   ┆ 3   ┆ 6   ┆ 2   ┆ 15  │\n",
       "│ 2006-01-03 ┆ Johor ┆ 2   ┆ 2   ┆ 4   ┆ 0   ┆ 8   │\n",
       "│ 2006-01-04 ┆ Johor ┆ 7   ┆ 11  ┆ 12  ┆ 3   ┆ 33  │\n",
       "│ 2006-01-05 ┆ Johor ┆ 3   ┆ 8   ┆ 8   ┆ 1   ┆ 20  │\n",
       "│ 2006-01-06 ┆ Johor ┆ 2   ┆ 0   ┆ 2   ┆ 0   ┆ 4   │\n",
       "│ 2006-01-07 ┆ Johor ┆ 1   ┆ 0   ┆ 0   ┆ 0   ┆ 1   │\n",
       "│ 2006-01-08 ┆ Johor ┆ 20  ┆ 27  ┆ 30  ┆ 9   ┆ 86  │\n",
       "│ 2006-01-09 ┆ Johor ┆ 0   ┆ 1   ┆ 1   ┆ 1   ┆ 3   │\n",
       "│ 2006-01-10 ┆ Johor ┆ 1   ┆ 0   ┆ 1   ┆ 0   ┆ 2   │\n",
       "└────────────┴───────┴─────┴─────┴─────┴─────┴─────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('blood_donations.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb3a57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>state</th><th>a</th><th>b</th><th>o</th><th>ab</th><th>all</th><th>weekday</th><th>week</th><th>month</th><th>day_of_year</th></tr><tr><td>date</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i8</td><td>i8</td><td>i8</td><td>i16</td></tr></thead><tbody><tr><td>2006-01-01</td><td>&quot;Johor&quot;</td><td>19</td><td>20</td><td>45</td><td>3</td><td>87</td><td>7</td><td>52</td><td>1</td><td>1</td></tr><tr><td>2006-01-02</td><td>&quot;Johor&quot;</td><td>4</td><td>3</td><td>6</td><td>2</td><td>15</td><td>1</td><td>1</td><td>1</td><td>2</td></tr><tr><td>2006-01-03</td><td>&quot;Johor&quot;</td><td>2</td><td>2</td><td>4</td><td>0</td><td>8</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>2006-01-04</td><td>&quot;Johor&quot;</td><td>7</td><td>11</td><td>12</td><td>3</td><td>33</td><td>3</td><td>1</td><td>1</td><td>4</td></tr><tr><td>2006-01-05</td><td>&quot;Johor&quot;</td><td>3</td><td>8</td><td>8</td><td>1</td><td>20</td><td>4</td><td>1</td><td>1</td><td>5</td></tr><tr><td>2006-01-06</td><td>&quot;Johor&quot;</td><td>2</td><td>0</td><td>2</td><td>0</td><td>4</td><td>5</td><td>1</td><td>1</td><td>6</td></tr><tr><td>2006-01-07</td><td>&quot;Johor&quot;</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>6</td><td>1</td><td>1</td><td>7</td></tr><tr><td>2006-01-08</td><td>&quot;Johor&quot;</td><td>20</td><td>27</td><td>30</td><td>9</td><td>86</td><td>7</td><td>1</td><td>1</td><td>8</td></tr><tr><td>2006-01-09</td><td>&quot;Johor&quot;</td><td>0</td><td>1</td><td>1</td><td>1</td><td>3</td><td>1</td><td>2</td><td>1</td><td>9</td></tr><tr><td>2006-01-10</td><td>&quot;Johor&quot;</td><td>1</td><td>0</td><td>1</td><td>0</td><td>2</td><td>2</td><td>2</td><td>1</td><td>10</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 11)\n",
       "┌────────────┬───────┬─────┬─────┬───┬─────────┬──────┬───────┬─────────────┐\n",
       "│ date       ┆ state ┆ a   ┆ b   ┆ … ┆ weekday ┆ week ┆ month ┆ day_of_year │\n",
       "│ ---        ┆ ---   ┆ --- ┆ --- ┆   ┆ ---     ┆ ---  ┆ ---   ┆ ---         │\n",
       "│ date       ┆ str   ┆ i64 ┆ i64 ┆   ┆ i8      ┆ i8   ┆ i8    ┆ i16         │\n",
       "╞════════════╪═══════╪═════╪═════╪═══╪═════════╪══════╪═══════╪═════════════╡\n",
       "│ 2006-01-01 ┆ Johor ┆ 19  ┆ 20  ┆ … ┆ 7       ┆ 52   ┆ 1     ┆ 1           │\n",
       "│ 2006-01-02 ┆ Johor ┆ 4   ┆ 3   ┆ … ┆ 1       ┆ 1    ┆ 1     ┆ 2           │\n",
       "│ 2006-01-03 ┆ Johor ┆ 2   ┆ 2   ┆ … ┆ 2       ┆ 1    ┆ 1     ┆ 3           │\n",
       "│ 2006-01-04 ┆ Johor ┆ 7   ┆ 11  ┆ … ┆ 3       ┆ 1    ┆ 1     ┆ 4           │\n",
       "│ 2006-01-05 ┆ Johor ┆ 3   ┆ 8   ┆ … ┆ 4       ┆ 1    ┆ 1     ┆ 5           │\n",
       "│ 2006-01-06 ┆ Johor ┆ 2   ┆ 0   ┆ … ┆ 5       ┆ 1    ┆ 1     ┆ 6           │\n",
       "│ 2006-01-07 ┆ Johor ┆ 1   ┆ 0   ┆ … ┆ 6       ┆ 1    ┆ 1     ┆ 7           │\n",
       "│ 2006-01-08 ┆ Johor ┆ 20  ┆ 27  ┆ … ┆ 7       ┆ 1    ┆ 1     ┆ 8           │\n",
       "│ 2006-01-09 ┆ Johor ┆ 0   ┆ 1   ┆ … ┆ 1       ┆ 2    ┆ 1     ┆ 9           │\n",
       "│ 2006-01-10 ┆ Johor ┆ 1   ┆ 0   ┆ … ┆ 2       ┆ 2    ┆ 1     ┆ 10          │\n",
       "└────────────┴───────┴─────┴─────┴───┴─────────┴──────┴───────┴─────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.with_columns([\n",
    "    pl.col(\"date\").dt.weekday().alias(\"weekday\"),\n",
    "    pl.col(\"date\").dt.week().alias(\"week\"),\n",
    "    pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "    pl.col(\"date\").dt.ordinal_day().alias(\"day_of_year\")   \n",
    "])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ca0d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>all</th><th>weekday</th><th>week</th><th>month</th><th>day_of_year</th></tr><tr><td>date</td><td>i64</td><td>i8</td><td>i8</td><td>i8</td><td>i16</td></tr></thead><tbody><tr><td>2006-01-01</td><td>525</td><td>7</td><td>52</td><td>1</td><td>1</td></tr><tr><td>2006-01-02</td><td>227</td><td>1</td><td>1</td><td>1</td><td>2</td></tr><tr><td>2006-01-03</td><td>112</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>2006-01-04</td><td>391</td><td>3</td><td>1</td><td>1</td><td>4</td></tr><tr><td>2006-01-05</td><td>582</td><td>4</td><td>1</td><td>1</td><td>5</td></tr><tr><td>2006-01-06</td><td>324</td><td>5</td><td>1</td><td>1</td><td>6</td></tr><tr><td>2006-01-07</td><td>118</td><td>6</td><td>1</td><td>1</td><td>7</td></tr><tr><td>2006-01-08</td><td>795</td><td>7</td><td>1</td><td>1</td><td>8</td></tr><tr><td>2006-01-09</td><td>346</td><td>1</td><td>2</td><td>1</td><td>9</td></tr><tr><td>2006-01-10</td><td>2</td><td>2</td><td>2</td><td>1</td><td>10</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 6)\n",
       "┌────────────┬─────┬─────────┬──────┬───────┬─────────────┐\n",
       "│ date       ┆ all ┆ weekday ┆ week ┆ month ┆ day_of_year │\n",
       "│ ---        ┆ --- ┆ ---     ┆ ---  ┆ ---   ┆ ---         │\n",
       "│ date       ┆ i64 ┆ i8      ┆ i8   ┆ i8    ┆ i16         │\n",
       "╞════════════╪═════╪═════════╪══════╪═══════╪═════════════╡\n",
       "│ 2006-01-01 ┆ 525 ┆ 7       ┆ 52   ┆ 1     ┆ 1           │\n",
       "│ 2006-01-02 ┆ 227 ┆ 1       ┆ 1    ┆ 1     ┆ 2           │\n",
       "│ 2006-01-03 ┆ 112 ┆ 2       ┆ 1    ┆ 1     ┆ 3           │\n",
       "│ 2006-01-04 ┆ 391 ┆ 3       ┆ 1    ┆ 1     ┆ 4           │\n",
       "│ 2006-01-05 ┆ 582 ┆ 4       ┆ 1    ┆ 1     ┆ 5           │\n",
       "│ 2006-01-06 ┆ 324 ┆ 5       ┆ 1    ┆ 1     ┆ 6           │\n",
       "│ 2006-01-07 ┆ 118 ┆ 6       ┆ 1    ┆ 1     ┆ 7           │\n",
       "│ 2006-01-08 ┆ 795 ┆ 7       ┆ 1    ┆ 1     ┆ 8           │\n",
       "│ 2006-01-09 ┆ 346 ┆ 1       ┆ 2    ┆ 1     ┆ 9           │\n",
       "│ 2006-01-10 ┆ 2   ┆ 2       ┆ 2    ┆ 1     ┆ 10          │\n",
       "└────────────┴─────┴─────────┴──────┴───────┴─────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with all\n",
    "df_all = df.group_by(['date']).agg([\n",
    "    pl.sum('all').alias('all'),\n",
    "    pl.first('weekday').alias('weekday'),\n",
    "    pl.first('week').alias('week'),\n",
    "    pl.first('month').alias('month'),\n",
    "    pl.first('day_of_year').alias('day_of_year')\n",
    "]).sort('date')\n",
    "\n",
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b27107",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"all\", \"weekday\", \"week\", \"month\", \"day_of_year\"]\n",
    "feature_data = df_all.select(feature_cols).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5ee019",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 7\n",
    "donations_array = df_all.drop(['date']).to_numpy()\n",
    "X, y = ml_utils.create_sequences_for_lstm(donations_array, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad29f054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6003, 7, 5) (6003,)\n",
      "(706, 7, 5) (706,)\n",
      "(354, 7, 5) (354,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = ml_utils.create_train_val_test(\n",
    "    X, y, train_frac=0.85, val_frac=0.1\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b4f7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = ml_utils.get_or_create_mlflow_experiment(\"MalaysiaBloodDonations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9123cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl.logging \n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46922f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with units=125, activation=relu, optimizer=adam, dropout=0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Validation Loss: 183526.375, Validation MAE: 284.10784912109375\n",
      "Running experiment with units=125, activation=relu, optimizer=adam, dropout=0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Validation Loss: 213515.859375, Validation MAE: 311.9988708496094\n",
      "Running experiment with units=125, activation=relu, optimizer=adam, dropout=0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "Validation Loss: 201352.8125, Validation MAE: 301.1484069824219\n",
      "Running experiment with units=125, activation=relu, optimizer=rmsprop, dropout=0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Validation Loss: 200111.0, Validation MAE: 299.67156982421875\n",
      "Running experiment with units=125, activation=relu, optimizer=rmsprop, dropout=0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D1B1DDA7A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D1B1DDA7A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Validation Loss: 193374.140625, Validation MAE: 290.1282653808594\n",
      "Running experiment with units=125, activation=relu, optimizer=rmsprop, dropout=0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D1BC0AC180> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D1BC0AC180> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Validation Loss: 213586.671875, Validation MAE: 318.6155090332031\n",
      "Running experiment with units=125, activation=relu, optimizer=adagrad, dropout=0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/11 21:44:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:46 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:46 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:48 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:48 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:49 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:49 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:51 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:54 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:55 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:56 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:57 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:58 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:44:59 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:00 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:06 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:06 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:08 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:09 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:10 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:15 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:16 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:20 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Validation Loss: 243134.6875, Validation MAE: 360.8981628417969\n",
      "Running experiment with units=125, activation=relu, optimizer=adagrad, dropout=0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/11 21:45:46 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:48 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:49 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:50 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:50 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:51 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:53 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:54 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:55 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:55 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:56 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:57 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:58 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:58 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:45:59 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:00 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:01 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:02 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:03 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:06 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:07 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:08 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:09 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:09 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:10 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n",
      "2025/05/11 21:46:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to synchronously create dataset (name already exists)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dropout \u001b[38;5;129;01min\u001b[39;00m dropout_list:    \n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning experiment with units=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, activation=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, optimizer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, dropout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mml_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_id\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\ml_utils.py:66\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, units, activation, dropout, optimizer, experiment_id)\u001b[39m\n\u001b[32m     61\u001b[39m early_stopping = create_early_stopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m15\u001b[39m)\n\u001b[32m     63\u001b[39m model = build_model(X_train.shape[\u001b[32m1\u001b[39m:], units=units, activation=activation, dropout=dropout,\n\u001b[32m     64\u001b[39m                             optimizer=optimizer, loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m val_loss = model.evaluate(X_val, y_val, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Validation MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:483\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m call_original = update_wrapper_extended(call_original, original)\n\u001b[32m    481\u001b[39m event_logger.log_patch_function_start(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m session.state = \u001b[33m\"\u001b[39m\u001b[33msucceeded\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m event_logger.log_patch_function_success(args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:182\u001b[39m, in \u001b[36mwith_managed_run.<locals>.patch_with_managed_run\u001b[39m\u001b[34m(original, *args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     managed_run = create_managed_run()\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     result = \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m managed_run:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\mlflow\\tensorflow\\__init__.py:1378\u001b[39m, in \u001b[36mautolog.<locals>._patched_inference\u001b[39m\u001b[34m(original, inst, *args, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m         shutil.rmtree(log_dir.location)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1376\u001b[39m     \u001b[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[32m   1377\u001b[39m     \u001b[38;5;66;03m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\mlflow\\tensorflow\\__init__.py:1349\u001b[39m, in \u001b[36mautolog.<locals>._patched_inference\u001b[39m\u001b[34m(original, inst, *args, **kwargs)\u001b[39m\n\u001b[32m   1343\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1344\u001b[39m         _logger.warning(\n\u001b[32m   1345\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFailed to log training dataset information to MLflow Tracking. Reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1346\u001b[39m             e,\n\u001b[32m   1347\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m history = \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n\u001b[32m   1352\u001b[39m     _log_keras_model(history, args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:474\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[39m\u001b[34m(*og_args, **og_kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         original_result = original(*_og_args, **_og_kwargs)\n\u001b[32m    472\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:425\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[39m\u001b[34m(original_fn, og_args, og_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    423\u001b[39m     event_logger.log_original_function_start(og_args, og_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     original_fn_result = \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m     event_logger.log_original_function_success(og_args, og_kwargs)\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:471\u001b[39m, in \u001b[36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[39m\u001b[34m(*_og_args, **_og_kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001b[32m    468\u001b[39m     disable_warnings=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    469\u001b[39m     reroute_warnings=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    470\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     original_result = \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaspe\\CodingProjects\\DataScienceProjects\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "units_list = [125, 250, 500, 1000]\n",
    "activation_list = [\"relu\"]\n",
    "optimizer_list = [\"adam\", \"rmsprop\", \"adagrad\"]\n",
    "dropout_list = [0.05, 0.1, 0.2]\n",
    "\n",
    "mlflow.tensorflow.autolog(log_models=True, log_datasets=False)\n",
    "for units in units_list:\n",
    "    for activation in activation_list:\n",
    "        for optimizer in optimizer_list:\n",
    "            for dropout in dropout_list:    \n",
    "                print(f\"Running experiment with units={units}, activation={activation}, optimizer={optimizer}, dropout={dropout}\")\n",
    "                ml_utils.run_experiment(\n",
    "                    X_train, y_train, X_val, y_val,\n",
    "                    units, activation, dropout, optimizer, experiment_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test, label='Actual Donations')\n",
    "plt.plot(model.predict(X_test), label='Predicted Donations')\n",
    "plt.title(\"LSTM Predictions vs. Actual (With Dates)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Donations\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16791132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
